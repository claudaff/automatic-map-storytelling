# Automatic Map Storytelling with Generative Pre-trained Transformer (GPT) Models
## Description
Maps provide information and knowledge about the world. However, the diversity of map types as well as the wide range of map styles and thematic/temporal contexts make it challenging for non-experts in the field (i.e., non-cartographers) to easily identify and understand maps. This is especially true for historical maps, as they often feature non-standard projections as well as hand-drawn styles and are usually more artistic than modern maps. Even though state-of-the-art image captioning methods such as [CLIP](https://github.com/openai/CLIP?tab=readme-ov-file) and [ClipCap](https://github.com/rmokady/CLIP_prefix_caption?tab=readme-ov-file) are promising, for historical maps, they generate captions that are either too simple, too general, or even wrong.

To address these challenges, we propose a decision tree structure based approach, combining keyword captions generated by fine-tuned CLIP models and the generative abilities of [GPT-3.5](https://platform.openai.com/docs/models/gpt-3-5). Our results demonstrate that our method is capable of overall outperforming the two state-of-the-art image captioning methods CLIP and CliPCap in telling the story of historical maps.

## Approach

![overview](https://github.com/claudaff/automatic-map-storytelling/assets/145538566/9ec92ba1-764b-48df-aabf-ddb52504006d)



We downloaded maps and their metadata from the online map repository [David Rumsey Map Collection](https://www.davidrumsey.com/). For training, we processed the metadata and generated ground-truth captions about "where", "what" and "when" to fine-tune separate CLIP models. During inference, the input map follows the structure of our proposed decision tree, where at the tree’s root node, first, the map type is determined. Then, keyword captions with respect to this map type are generated. At last, a GPT model is used to merge the generated keywords and extend the map’s story about the "why" component. Built on that, we developed a web application for interactive map storytelling.

## Reproduction
### 1. Training prerequisites

```sh
git clone https://github.com/claudaff/automatic-map-storytelling.git
```

### 2. Map datasets

Download and unzip the following fifteen .zip files containing our collected maps with associated metadata. (Overall less than 2 GB).

[M1](https://drive.google.com/file/d/1EWVyhGqqPq-9bQUSOFxBd-L3zaVjfbbl/view?usp=drive_link), 
[M2](https://drive.google.com/file/d/1ZV-0CT_9Nh21yLHyajoVsGyZKywo03UB/view?usp=drive_link)
[M3](https://drive.google.com/file/d/11XBnAgegMf-jWNlMAStL4w_U3CWCuAD5/view?usp=drive_link), 
[M4](https://drive.google.com/file/d/1SoZGjEao8B0j9B0kBu79GxsUMg-gjCW1/view?usp=drive_link), 
[M5](https://drive.google.com/file/d/1FGNIDbX1Js5Wjv7vaRUy6PRo7-bD2D0K/view?usp=drive_link), 
[M6](https://drive.google.com/file/d/1GT6Ulfr1cR9CXuTbfXLKqzkokD00MV8z/view?usp=drive_link), 
[M7](https://drive.google.com/file/d/14_u9gn3nwjOQHaokB9gT-dV8nYF5YMOW/view?usp=drive_link), 
[M8](https://drive.google.com/file/d/1xjyaI4xaKWzk1ODERfAwMFhhUIWw1deM/view?usp=drive_link), 
[M9](https://drive.google.com/file/d/1nBRwbnYcDk4feWYCSXtEUh3qVrfmdA7l/view?usp=drive_link), 
[M10](https://drive.google.com/file/d/1S7NFe8zjyOH3IMWFtQH8EzseE0VIQSm4/view?usp=drive_link), 
[M11](https://drive.google.com/file/d/1o3XjaPnexo0ZUh2kB-HVLCsgxMJzBkeF/view?usp=drive_link), 
[M12](https://drive.google.com/file/d/1C3KnB_P9XAyn2ou6Vb3KuvMzszCTvGN0/view?usp=drive_link), 
[M13](https://drive.google.com/file/d/1i3REduWyjhef9lXF6RuWuWIvSDif-Gxz/view?usp=drive_link), 
[M14](https://drive.google.com/file/d/1dcXKBu4rgtkZXJSOhpGYnpA43UrCwj_5/view?usp=drive_link), 
[M15](https://drive.google.com/file/d/1H_4D-I1EKuF8ggXIRLNjxQkf-GJQExot/view?usp=drive_link)

### 3. Generate ground-truth captions

Run 'CaptionGenerationClassical.py' and 'CaptionGenerationPictorial.py'.

### 4. CLIP fine-tuning
